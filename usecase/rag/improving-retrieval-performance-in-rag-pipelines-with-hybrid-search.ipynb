{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook originates from an article that seems like a simple replacement piece, but in reality, there are many pitfalls to be aware of, especially if you're using Azure OpenAI instead of OpenAI. For more detail: https://towardsdatascience.com/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5#6b8c ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key parts that need to be adapted\n",
    "\n",
    "**I must say, the integration of Weaviate's hybrid search client with Azure OpenAI is designed very counterintuitively**, And as always, if you have GPT-4, then please discard GPT-3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(),\n",
    "    additional_headers={\n",
    "        \"X-Azure-Api-Key\": os.getenv(\"AZURE_EMBED_API_KEY\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "class_obj = {\n",
    "    \"class\": \"LangChain\",\n",
    "    \"vectorizer\": \"text2vec-openai\",\n",
    "    \"moduleConfig\": {\n",
    "        \"text2vec-openai\": { # One of the most foolish API designs I've ever seen. Why we need resourceName and deploymentId here?\n",
    "            \"baseURL\": os.getenv(\"AZURE_EMBED_ENDPOINT\"),\n",
    "            \"resourceName\": os.getenv(\"AZURE_EMBED_RESOURCE_NAME\"),\n",
    "            \"deploymentId\": \"text-embedding-ada-002\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "client.schema.delete_all()\n",
    "client.schema.create_class(class_obj)\n",
    "\n",
    "from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever\n",
    "\n",
    "retriever = WeaviateHybridSearchRetriever(\n",
    "    alpha=0.5,  # defaults to 0.5, which is equal weighting between keyword and semantic search\n",
    "    client=client,  # keyword arguments to pass to the Weaviate client\n",
    "    index_name=\"LangChain\",  # The name of the index to use\n",
    "    text_key=\"text\",  # The name of the text key to use\n",
    "    attributes=[],  # The attributes to return in the results\n",
    ")\n",
    "\n",
    "retriever.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The president honored Justice Stephen Breyer for his service as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. He thanked Justice Breyer for his service to the country. The president also mentioned that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to continue Justice Breyerâ€™s legacy of excellence.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = AzureChatOpenAI(azure_deployment=\"gpt-4\", openai_api_version=\"2023-08-01-preview\", temperature=0) # If you're using GPT-3.5 here, then the results are very unpredictable.\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "rag_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
