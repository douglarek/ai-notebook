{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reproduction from the video: \"Building a Research Assistant from Scratch\" (https://www.youtube.com/watch?v=DjuXACWYkkU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "\n",
    "The simplest example is to retrieve text from a webpage, and then summarize its main content to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question is asking for an explanation of what LangSmith is.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "template = \"\"\"Summarize the following question based on the context:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\"\n",
    "\n",
    "\n",
    "url = \"https://blog.langchain.dev/announcing-langsmith/\"\n",
    "\n",
    "page_content = scrape_text(url)[:10000]\n",
    "llm = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo-16k\", openai_api_version=\"2023-08-01-preview\", temperature=0)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is langsmith\",\n",
    "        \"context\": page_content\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/7c48f66b-2ec3-4bfd-b043-d8e1baa12bfb/r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "\n",
    "Using the prompt from the gpt-research repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a unified platform that helps developers debug, test, evaluate, and monitor their LLM (Large Language Model) applications. It aims to bridge the gap between prototype and production by addressing issues such as application performance. LangSmith provides visibility into model inputs and outputs, allows for experimentation with chains and prompt templates, helps identify and resolve issues like unexpected results or latency problems, and offers tools for dataset creation and evaluation. It also enables monitoring of system-level performance, model/chain performance, and user interaction with the application. LangSmith has been tested and used by various companies and organizations to improve their LLM-powered applications.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUMMARY_TEMPLATE = \"\"\"{text} \n",
    "-----------\n",
    "Using the above text, answer in short the following question: \n",
    "> {question}\n",
    "-----------\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available.\"\"\"  # noqa: E501\n",
    "SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n",
    "\n",
    "chain = SUMMARY_PROMPT | llm | StrOutputParser()\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"text\": page_content,\n",
    "        \"question\": \"what is langsmith\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/403bcb28-e149-4d67-95cc-2efa825fe354/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "\n",
    "Refactor using RunnablePassthrough to accept a URL and question, and retrieve text from the URL to summarize and answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a unified platform that helps developers debug, test, evaluate, and monitor their LLM (Large Language Model) applications. It aims to bridge the gap between prototype and production by providing tools and features to address common challenges faced by LLM application developers. LangSmith offers visibility into model inputs and outputs, allows for experimentation with chains and prompt templates, helps identify issues such as unexpected results or latency problems, and provides features for dataset creation, testing, and evaluation. It also enables monitoring of system-level performance, model/chain performance, and user interactions. LangSmith has been tested and used by various companies and organizations, including Snowflake, Boston Consulting Group, DeepLearningAI, Klarna, Mendable, Multi-On, and Quivr.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000],\n",
    ") | SUMMARY_PROMPT | llm | StrOutputParser()\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"url\": url,\n",
    "        \"question\": \"what is langsmith\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/24dc2e21-02ff-4fbb-98b9-3e4ea7da38b4/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "\n",
    "On DuckDuckGo, conduct a search based on a specific question and return a set number of links. This process has evolved to: for a given question, first search on DuckDuckGo to obtain the required number of links, then extract text from these links and compile the information to answer the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangSmith is a product developed by the creators of LangChain, a popular language model software tool. It aims to address the challenges of deploying and maintaining large language models (LLMs) in production. LangSmith focuses on providing features for reliability, such as debugging, testing, evaluating, monitoring, and usage metrics. It also offers a user-friendly UI to visualize and understand LLM workflows. While there may be potential competitors in the market, LangSmith aims to differentiate itself by integrating with other tools and providing value-add features. The author suggests that LangSmith should focus on extensibility to be more impactful and competitive in the long run.',\n",
       " 'LangSmith is a platform that works in tandem with LangChain and other LLM frameworks. It is used for tracing and debugging LLM applications and intelligent agents. LangSmith allows you to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. It provides a visualization of the inputs and outputs of LLM calls, helping you understand the behavior of your LLM programs. You can use LangSmith to identify issues like unexpected results, looping agents, and slow chains. It also provides a Playground feature that allows interactive editing of inputs, model selection, parameter adjustment, and more. LangSmith is a valuable tool for moving LLM applications from prototype to production.',\n",
       " 'LangSmith is a tool developed by the team behind Langchain, which is a popular software tool for building prototypes with large language models (LLMs). While Langchain focuses on prototyping, LangSmith is designed to help developers take their LLM applications into production in a reliable and maintainable way. It addresses challenges such as debugging, testing, evaluating, monitoring, and usage metrics for LLM applications. LangSmith provides a simple and intuitive UI that reduces the barrier to entry for developers without a software background. It also offers integrations with OpenAI evals and fine-tuning providers, allowing developers to export data and train on it directly.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "RESULTS_PER_QUESTION = 3\n",
    "\n",
    "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "def web_search(query: str, num_results: int = RESULTS_PER_QUESTION):\n",
    "    results = ddg_search.results(query, num_results)\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000],\n",
    ") | SUMMARY_PROMPT | AzureChatOpenAI(azure_deployment=\"gpt-35-turbo-16k\", openai_api_version=\"2023-08-01-preview\") | StrOutputParser()\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"])\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is langsmith\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/cf28125b-f920-49fa-a1e7-9d7bd991ac66/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5\n",
    "\n",
    "Let LLM answer how to use what kind of alternative questions to search engine retrieval for a specific question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Langsmith vs Langchain differences',\n",
       " 'Comparison between Langsmith and Langchain',\n",
       " 'Langsmith and Langchain: contrasting features']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Write 3 google search queries to search online that form an \"\n",
    "            \"objective opinion from the following: {question}\\n\"\n",
    "            \"You must respond with a list of strings in the following format: \"\n",
    "            '[\"query 1\", \"query 2\", \"query 3\"].',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "search_question_chain = SEARCH_PROMPT | llm | StrOutputParser() | json.loads\n",
    "\n",
    "search_question_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langsmith and langchain\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/061fe805-6e9c-4c8b-82c8-a4ca1c18fa14/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The given text does not mention anything about \"Langsmith.\" Therefore, there is no information available to compare Langsmith and LangChain.',\n",
       "  \"LangSmith is a new product created by the team behind LangChain, a popular software tool for large language models (LLMs). LangChain was initially designed for building prototypes, while LangSmith focuses on helping developers bring LLM applications into production in a reliable and maintainable way. LangSmith addresses challenges related to reliability by providing features in the areas of debugging, testing, evaluating, monitoring, and usage metrics. Its intuitive UI reduces the barrier to entry for developers without a software background and allows for visualizations of the LLM system's processes. While there are no direct competitors, organizations like Vercel and embeddings providers may develop similar tooling in the future. LangSmith aims to connect with various tools and integrate with platforms such as OpenAI evals and fine-tuning providers. The author suggests that LangSmith should focus on extensibility and expand in scope to remain competitive in the long term. The text does not explicitly state the differences between LangSmith and LangChain.\",\n",
       "  \"LangSmith is a framework built on the shoulders of LangChain. While LangChain is an open source tool for developing LLM applications, LangSmith is a complementary platform for managing and understanding the inner workings of LLMs and AI agents. LangSmith allows users to debug, monitor, test, evaluate, and collaborate on their LLM applications. It provides tools for debugging agent loops, testing applications, evaluating performance, and monitoring AI outputs. Traces in LangSmith are similar to logs in programming and allow users to track the decisions made by LLMs and AI agents. LangSmith has been used by CommandBar to optimize and improve their AI-powered chatbot, HelpHub. By integrating LangSmith, CommandBar was able to capture traces and fine-tune their chatbot's functionality. They have traced over X0 million tokens with LangSmith and have seen improvements in user experience. Overall, LangSmith is a valuable tool for understanding and improving AI decision-making in LLM applications.\"],\n",
       " ['The text does not provide a direct comparison between LangSmith and LangChain. It mentions that LangChain provides frameworks and utilities for orchestration, debugging, and connecting to data sources, while LangSmith provides tools for logging, debugging, testing, and collaboration for developing context-aware reasoning applications. However, it does not provide a detailed comparison of the two tools.',\n",
       "  \"LangSmith and LangChain are both powerful tools for testing and evaluating language models and AI applications. However, there are some key differences between the two.\\n\\nLangChain is a platform for creating prototypes of language models and AI applications. It provides tracing tools that allow developers to investigate and debug the execution steps of an agent. This helps in understanding the workflow and building confidence in the accuracy of the model's responses.\\n\\nOn the other hand, LangSmith is a dynamic testing framework built on top of LangChain. It is specifically designed for evaluating language models and AI applications in production-grade environments. LangSmith offers features such as quick debugging, customizable test scenarios, metrics and analytics, and interactive visualization. These features help developers refine and enhance their models for real-world usage scenarios.\\n\\nIn summary, LangChain is used for creating prototypes and investigating model execution steps, while LangSmith is used for testing and evaluating language models in production environments.\",\n",
       "  'LangSmith is the latest product from the creators of LangChain, a popular large language model (LLM) software tool. LangChain was designed for building prototypes, while LangSmith focuses on getting LLM applications into production and ensuring reliability and maintainability. LangSmith provides features in five core areas: debugging, testing, evaluating, monitoring, and usage metrics. It offers a simple and intuitive UI, making it accessible to developers without a software background. While there are no direct competitors to LangSmith currently, platforms like Vercel may develop similar tooling in the future. LangSmith aims to connect with other tools and providers through integrations and offers extensibility for developers. The long-term success of LangSmith will depend on its ability to expand and compete with other providers and tooling ecosystems.'],\n",
       " [\"LangSmith and LangChain are two related frameworks that serve different purposes in the field of AI and language models.\\n\\nLangSmith is a dynamic testing framework that allows developers to evaluate language models and AI applications. It provides tools for analyzing model responses and extracting valuable insights, which helps in refining and enhancing the models for real-world usage scenarios. LangSmith offers features such as quick debugging, customizable test scenarios, metrics and analytics, and interactive visualization.\\n\\nOn the other hand, LangChain is a platform for building production-grade language model applications. It provides tracing tools that help in investigating and debugging an agent's execution steps. The LangChain tracing tool allows users to visualize the sequence of calls, which aids in understanding the model's inner workings and building confidence in its responses. LangChain is used for creating prototypes and offers more in-depth analysis of the agent's workflow.\\n\\nIn summary, LangSmith is for testing and evaluating language models, while LangChain is for building production-grade applications and analyzing the agent's execution steps. Both frameworks complement each other and contribute to the development and refinement of AI applications.\",\n",
       "  'LangChain is an open-source tool that makes it easy to develop language model applications. LangSmith is a framework built on top of LangChain that allows users to track the inner workings of language models and AI agents. LangSmith provides debugging, testing, evaluating, and monitoring capabilities for AI decision-making. Traces in LangSmith are similar to logs in programming and allow users to see the text input and output of language models. LangChain focuses on developing language model applications, while LangSmith focuses on understanding and managing the decision-making process of these models.',\n",
       "  'The text does not provide any information about the contrasting features between Langsmith and Langchain.']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"])\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()\n",
    "\n",
    "chain = search_question_chain | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langsmith and langchain\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/eb43427c-5667-4ef0-933c-2d5447c08017/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Report: The Difference Between LangSmith and LangChain\\n\\n## Introduction\\n\\nLangSmith and LangChain are two AI frameworks developed by the same team. While both tools are used for testing and evaluating language models and AI applications, they have distinct features and purposes. This report aims to provide a detailed analysis of the differences between LangSmith and LangChain based on the information provided.\\n\\n## LangSmith: A Framework for Production-Grade Applications\\n\\nLangSmith is a new product created by the team behind LangChain. It is specifically designed to address the challenges of getting language model (LLM) applications into production in a reliable and maintainable way. Unlike LangChain, which focuses on prototyping, LangSmith aims to provide features that assist developers in building production-grade LLM applications.\\n\\n### Features of LangSmith\\n\\nLangSmith offers a range of features that are essential for developing and managing LLM applications. These features include:\\n\\n1. Debugging: LangSmith provides tools for quick debugging, allowing developers to identify and fix issues in their applications efficiently.\\n\\n2. Testing: The framework offers customizable test scenarios, enabling developers to thoroughly test their LLM applications and ensure their accuracy and performance.\\n\\n3. Evaluating: LangSmith allows developers to evaluate the performance of their LLM applications, providing valuable insights into their capabilities and areas for improvement.\\n\\n4. Monitoring: The framework offers monitoring capabilities, enabling developers to track the performance and behavior of their LLM applications in real-time.\\n\\n5. Usage Metrics: LangSmith provides metrics and analytics that help developers understand how their LLM applications are being used and identify patterns or trends.\\n\\n6. User Interface: LangSmith offers a simple and intuitive user interface (UI), reducing the barrier to entry for developers without a software background.\\n\\n7. Extensibility: Developers have expressed the need for LangSmith to be extensible, allowing its core features to be integrated into other applications and services.\\n\\n### Competitors and Future Scope\\n\\nLangSmith's competitors include platforms like Vercel, which may also develop similar tooling for LLM applications. To remain competitive and succeed in the long term, LangSmith should continue to expand its scope and connect with as many tools as possible. This will allow developers to leverage LangSmith's features while integrating it seamlessly into their existing workflows.\\n\\n## LangChain: A Framework for Prototyping\\n\\nLangChain, on the other hand, is an open-source tool that focuses on the development and prototyping of LLM applications. It provides a framework and tools that enable developers to quickly build and test language models and AI applications.\\n\\n### Features of LangChain\\n\\nLangChain offers several features that are specifically tailored for prototyping LLM applications. These features include:\\n\\n1. Debugging: LangChain provides tracing tools that help developers investigate and debug the execution steps of an agent. These tools allow users to visualize the sequence of calls and gain a deeper understanding of the inner workings of the model.\\n\\n2. Visualization: The framework offers tools for visualizing networks, allowing developers to analyze and optimize the structure of their language models.\\n\\n3. Quality Control: LangChain provides tools for quality control, ensuring that the building blocks of different LLMs are working correctly and producing accurate results.\\n\\n4. Accessing Prompts and LLMs: LangChain allows developers to access and manipulate prompts and LLMs, providing flexibility in the development process.\\n\\n### LangChain and LangSmith: Complementary Tools\\n\\nLangChain and LangSmith are complementary tools that serve different purposes in the development and deployment of LLM applications. While LangChain focuses on prototyping and investigating the execution steps of agents, LangSmith is designed for building production-grade applications.\\n\\nDevelopers can use LangChain to quickly build and test language models and AI applications, gaining insights into their inner workings. Once the prototype is ready, LangSmith can be used to address the challenges of bringing the application into production. LangSmith provides features for debugging, testing, evaluating, monitoring, and usage metrics, ensuring the quality and reliability of LLM applications.\\n\\n## Conclusion\\n\\nIn conclusion, LangSmith and LangChain are two AI frameworks developed by the same team. While LangChain is focused on prototyping and investigating the execution steps of agents, LangSmith is designed for building production-grade LLM applications. LangSmith offers features such as debugging, testing, evaluating, monitoring, and usage metrics, all accessible through a simple and intuitive UI. On the other hand, LangChain provides tools for quick debugging, visualization of networks, quality control, and accessing prompts and LLMs.\\n\\nBoth LangSmith and LangChain have their own unique features and purposes, making them complementary tools in the development and deployment of LLM applications. Developers can use LangChain for prototyping and gaining insights into the inner workings of their models, while LangSmith helps address the challenges of bringing these applications into production.\\n\\nTo remain competitive and succeed in the long term, LangSmith should continue to expand its scope and connect with as many tools as possible, providing an extensible platform for developers. This will allow LangSmith to meet the evolving needs of developers and compete effectively with other providers and tooling ecosystems.\\n\\n## References\\n\\n- [LangSmith: A Framework for Production-Grade Applications](https://example.com/langsmith)\\n- [LangChain: A Framework for Prototyping](https://example.com/langchain)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_research_chain = search_question_chain | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()\n",
    "\n",
    "WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n",
    "\n",
    "\n",
    "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
    "RESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n",
    "--------\n",
    "{research_summary}\n",
    "--------\n",
    "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
    "The report should focus on the answer to the question, should be well structured, informative, \\\n",
    "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
    "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
    "You must write the report with markdown syntax.\n",
    "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
    "You must write the report in apa format.\n",
    "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def collapse_list_of_lists(list_of_lists):\n",
    "    content = []\n",
    "    for l in list_of_lists:\n",
    "        content.append(\"\\n\\n\".join(l))\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    research_summary= full_research_chain | collapse_list_of_lists\n",
    ") | prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langsmith and langchain\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/6e104657-b6e9-4ce9-8634-2bf350301517/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8\n",
    "\n",
    "The reply above cannot retrieve the URL of the quoted text because it wasn't used during the initial capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Report: The Difference Between LangSmith and LangChain\\n\\n## Introduction\\n\\nThe purpose of this report is to analyze and compare the differences between LangSmith and LangChain based on the information provided. LangSmith and LangChain are both products developed by the same team, but they serve different purposes in the context of large language models (LLMs) and AI applications. This report will provide an in-depth analysis of the features, functionalities, and target audiences of both LangSmith and LangChain.\\n\\n## LangSmith: A Reliable and Maintainable LLM Production Platform\\n\\nLangSmith is a new product developed by the creators of LangChain. While LangChain focuses on reducing the barrier to entry for building prototypes with LLMs, LangSmith aims to help LLM applications move into production in a reliable and maintainable way. It addresses challenges related to reliability by providing features for debugging, testing, evaluating, monitoring, and usage metrics. LangSmith also offers a simple and intuitive user interface (UI) for performing these tasks, making it more accessible for developers without a software background.\\n\\nOne of the key features of LangSmith is its focus on reliability. It provides tools for debugging, testing, and monitoring the inner workings of LLMs and AI agents within a product. This allows developers to identify and fix issues that may arise during the production phase. Additionally, LangSmith offers features for evaluating and tracking usage metrics, which can help developers optimize the performance of their LLM applications.\\n\\nLangSmith also offers integrations with OpenAI evals and fine-tuning providers, which adds value to the platform. These integrations allow developers to leverage the capabilities of these providers and enhance the functionality of their LLM applications. Furthermore, LangSmith provides the ability to visualize LLM processes and log historical data, which can be useful for analyzing and improving the performance of LLM applications.\\n\\nThe main ask from LangSmith is extensibility, allowing its core features to be built into other applications and services. This means that developers can integrate LangSmith into their existing workflows and leverage its functionalities to enhance their LLM applications. This extensibility makes LangSmith a versatile platform that can adapt to different use cases and requirements.\\n\\n## LangChain: A Tool for Prototyping LLM Applications\\n\\nLangChain, on the other hand, is a popular software tool for building prototypes with LLMs. It focuses on reducing the barrier to entry for developers who want to experiment and create proof-of-concept applications using LLMs. LangChain provides tracing tools that allow developers to investigate and debug the execution steps of AI agents. These tracing tools enable users to visualize the sequence of calls effectively, fostering a deeper understanding of the model's inner workings and building confidence in the accuracy of its responses.\\n\\nWhile LangChain is primarily used for prototyping, it is important to note that it serves as the foundation for LangSmith. LangSmith is built on top of LangChain and provides additional features and functionalities to support the production phase of LLM applications. This integration between LangSmith and LangChain allows developers to seamlessly transition from prototyping to production, ensuring a smooth and reliable development process.\\n\\n## Comparison: LangSmith vs. LangChain\\n\\nBased on the information provided, we can identify several key differences between LangSmith and LangChain:\\n\\n1. **Focus**: LangSmith is focused on helping LLM applications move into production in a reliable and maintainable way. It provides features for debugging, testing, evaluating, monitoring, and usage metrics. On the other hand, LangChain is primarily used for building prototypes of LLM applications. It offers tracing tools that allow developers to investigate and debug the execution steps of AI agents.\\n\\n2. **Target Audience**: LangSmith is designed for developers who want to bring their LLM applications into production. It offers a simple and intuitive UI, making it accessible for developers without a software background. LangChain, on the other hand, targets developers who want to experiment and create proof-of-concept applications using LLMs. It provides tracing tools that enable users to visualize the sequence of calls effectively.\\n\\n3. **Features**: LangSmith provides features for debugging, testing, evaluating, monitoring, and usage metrics. These features are essential for ensuring the reliability and performance of LLM applications in production. LangChain, on the other hand, focuses on providing tracing tools that allow developers to investigate and debug the execution steps of AI agents. These tools help developers gain a deeper understanding of the inner workings of LLMs and build confidence in the accuracy of their responses.\\n\\n4. **Integration**: LangSmith offers integrations with OpenAI evals and fine-tuning providers, which adds value to the platform. These integrations allow developers to leverage the capabilities of these providers and enhance the functionality of their LLM applications. LangChain, on the other hand, serves as the foundation for LangSmith. It provides the necessary tools and functionalities for building prototypes of LLM applications.\\n\\n5. **Extensibility**: LangSmith's main ask is extensibility, allowing its core features to be built into other applications and services. This means that developers can integrate LangSmith into their existing workflows and leverage its functionalities to enhance their LLM applications. LangChain, on the other hand, does not explicitly mention extensibility as a key feature.\\n\\n## Conclusion\\n\\nIn conclusion, LangSmith and LangChain are two interconnected frameworks developed by the same team. While LangChain focuses on building prototypes of LLM applications, LangSmith is designed to help developers bring these applications into production in a reliable and maintainable way. LangSmith provides features for debugging, testing, evaluating, monitoring, and usage metrics, while LangChain offers tracing tools for investigating and debugging the execution steps of AI agents.\\n\\nLangSmith's focus on reliability and its integrations with OpenAI evals and fine-tuning providers make it a valuable platform for building and maintaining LLM applications in production. On the other hand, LangChain's tracing tools provide developers with the necessary insights to understand the inner workings of LLMs and build confidence in the accuracy of their responses during the prototyping phase.\\n\\nBoth LangSmith and LangChain serve different purposes and target different audiences. While LangSmith is aimed at developers who want to bring their LLM applications into production, LangChain targets developers who want to experiment and create proof-of-concept applications using LLMs. The integration between LangSmith and LangChain allows for a seamless transition from prototyping to production, ensuring a smooth and reliable development process.\\n\\n## References\\n\\nNo references were provided in the given information.\\n\\nNote: The report is based solely on the information provided and does not include any external sources.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
    "    summary = RunnablePassthrough.assign(\n",
    "    text = lambda x: scrape_text(x[\"url\"])[:10000],\n",
    ") | SUMMARY_PROMPT | llm | StrOutputParser()) | (lambda x: f\"URL: {x['url']}\\n\\nSUMMARY: {x['summary']}\")\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langsmith and langchain\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# langsmith output: https://smith.langchain.com/public/e37286b3-500f-48e5-b245-369283b9a94c/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9\n",
    "\n",
    "Run a fastapi by Langserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sse_starlette\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/research-assistant\",\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)\n",
    "\n",
    "# open browser to http://localhost:8000/research-assistant/playground/ and play"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
